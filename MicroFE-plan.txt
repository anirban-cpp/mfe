#   Set up Git monorepo which will contain all our individual micro FEs.
    This can be done easily with turbo: "npx create-turbo@latest -e with-react-native-web"
    Link: https://github.com/vercel/turbo/tree/main/examples/with-react-native-web

#   Repo will have packages folder. This will have container (which is our main host app)
    Along with that we will have our individual microFEs and shared (state/store).
    Also we can additionally have UI package that may contain any component/sharable code
    from external teams like Phoenix (if needed)

#   Set up github actions scripts (CI/CD) that will look into each sub-project or microFE when we push our code.
    If any changes are made, then we will start a process that will make a production version of that microFE app with webpack

#   Once production build is done, we can upload all our files to Amazon S3

Each microFE step should be independent. So if microFE A is changed only, then only its production build is done and uploaded to S3. Others should be affected because of that
One can be deployed without others being deployed.

Amazon S3 will have the built version of all our different sub-projects/microFEs.

When a file request is made, the file is not served directly from S3, instead it will make a request to Amazon CloudFront (basically a CDN)
Basically Amazon CloudFront will see the incoming request and figure out which files to pull out of our S3 bucket and serve it back to the browser.


For the CI/CD pipeline setup
----------------------------
We can make use of github actions.
Basically whenever any github event is triggered say 'push', 'create PR', 'merge', etc, the webhook will run all workflows associated with this event


##  Workflow for deploying container(host)

Whenever code is pushed to the master branch (primary branch) and there is a commit that contains a change to the container/host folder, the following
steps need to be performed:

->  Change to container folder
->  Install the dependencies
->  Create a production build usig webpack
->  Upload the result to AWS S3

These commands need to be executed in the Github virtual machine by creating a .yml file in .github/workflows

/*
we will be writing our container workflow and using the chrislennon/action-aws-cli@v1.1 action. 
Unfortunately, this is now failing and appears to be no longer maintained.
 A community fork was created to fix the issues which we can use instead:

instead of this:

      - uses: chrislennon/action-aws-cli@v1.1

write this:

      - uses: shinyinc/action-aws-cli@v1.2

This updated action will require an AWS_DEFAULT_REGION key, so, for now, we can just add a placeholder.

      - uses: shinyinc/action-aws-cli@v1.2
      - run: aws s3 sync dist s3://${{ secrets.AWS_S3_BUCKET_NAME }}/container/latest
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ""
*/


Creating the S3 bucket
----------------------
Create the S3 bucket and change the setting to public as we want our app to be available in public domain.
Basically 
->  enable Static Website hosting
->  Hosting Type: Host a static website
->  index document: index.html
->  error document: index.html
->  Go to permissions and uncheck block all public access
->  To allow AWS CloudFront CDN, go to Permissions -> Bucket Policy -> Edit -> Policy Generator

Setup AWS CloudFront (CDN)
--------------------------
->  Search for CloudFront in AWS for Gloal CDN setup
->  Create Distribution
->  Select Web distribution
->  Fill in the details: 
    Origin Settings: Origin Domain Name, 
    Default Cache Behavior Settings: Set Viewer Protocol Policy to edirect HTTP to HTTPS
    Distribution Settngs: SSL Certificate -> Custom SSL Certificate for our yubi domain
-> Create distribution
-> Once the CloudFront distribution is created and the status is saying deployed, click on the distribution -> edit and go to 
    default Root Object and set it to "/container/latest/index.html"
-> Then go to the bottom and click on Yes Edit.
-> Also, go to Error Pages.
    Create Custom Error Response and set the HTTP Error Code to say 403 Forbidden.
    Customize Error Response to Yes
    Provide Response Path as "/container/latest/index.html"
    Change HTTP response code to 200 OK
    Ans click on create
-> Go to the General Tab. Take the Domain Name from there

Update for Generating Keys
--------------------------

Create an IAM user and then generate a key pair for deployment. There is a minor required change to this flow. Instead of being prompted to create a key pair during the IAM user creation, you must first create the IAM user, then, create a key pair associated with that user. AWS has also changed the terminology from Programmatic Access, to Command Line Interface (CLI).

Full updated instructions can be found below:

1. Search for "IAM"

2. Click "Create Individual IAM Users" and click "Manage Users"

3. Click "Add User"

4. Enter any name youâ€™d like in the "User Name" field.

5. Click "Next"

6. Click "Attach Policies Directly"

7. Use the search bar to find and tick AmazonS3FullAccess and CloudFrontFullAccess

8. Click "Next"

9. Click "Create user"

10. Select the IAM user that was just created from the list of users

11. Click "Security Credentials"

12. Scroll down to find "Access Keys"

13. Click "Create access key"

14. Select "Command Line Interface (CLI)"


15. Scroll down and tick the "I understand..." check box and click "Next"

16. Copy and/or download the Access Key ID and Secret Access Key to use for deployment.

Reminder on AWS_DEFAULT_REGION
A few lectures ago we mentioned the need to use a different action and left a placeholder for the AWS_DEFAULT_REGION key.

Let's make sure this gets set correctly now.

In the AWS Dashboard use the Services search bar to find S3 and load its dashboard. Once there, copy the AWS region listed to the right of your bucket:


Then, in your container.yml, paste in the value for the AWS_DEFAULT_REGION like so:

      - uses: shinyinc/action-aws-cli@v1.2
      - run: aws s3 sync dist s3://${{ secrets.AWS_S3_BUCKET_NAME }}/container/latest
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-2

For this go to AWS, and search for servide 'iam'
Go to Users
Give User name for the project's github actions
Give access type as 'Programmatic access'
Next to to Permissions
Add policy for S3 bucket access and cloudFront access
Finally create the user
Copy the Access Key ID and the Secret access key

->  With this Access key ID and Secret access key, go to github Secrets and Variables and create a new repository secret
    Provide the name and value as 'Access Key ID' and 'Secret access key'

-> The AWS_ACCESS_KEY_ID secret should have a name of AWS_ACCESS_KEY_ID and value of what was there for 'Access Key ID'
-> Same for AWS_SECRET_ACCESS_KEY
-> Also for AWS_S3_BUCKET_NAME

Caching issue with AWS CloudFront
---------------------------------

Since CloudFront does caching, it will not check for any file changes in our S# for deployment.
So we have to invalidate thta

->  Go to CloudFront Distributions.
->  Open up our distribution.
->  Create Invalidation.
->  give the path to the index.html file (/container/latest/index.html)
->  A better way is to automate this with our github workflow webhook.
    Command: aws cloudfront create-invalidation --distribution-id ${{ our distribution id }}

This "our distribution id" can be provided from our Github Secrets

Add PRODUCTION_DOMAIN secret to github secrets
Update github workflow and the webpack.prod.js config file such that the outputFile has a publicPath property: publicPath: '/appName/latest/'


Workflow
--------

-> Each team will push their features on their own git branches.
-> Once pushed they will raise a PR to the master branch.
-> Other engineers will review this
-> Once it is ready for deployment, merge this PR
-> Thr github workflow will notice a change in the master branch and will run the deployment script.

S3 Bucket Creation and Configuration
------------------------------------

S3 Bucket Creation and Configuration
Go to AWS Management Console and use the search bar to find S3

Click Create Bucket

Specify an AWS Region

Provide unique Bucket Name and click Create Bucket

Click the new Bucket you have created from the Bucket list.

Select Properties

Scroll down to Static website hosting and click Edit

Change to Enable

Enter index.html in the Index document field

Click Save changes

Select Permissions

Click Edit in Block all public access

Untick the Block all public access box.

Click Save changes

Type confirm in the field and click Confirm

Find the Bucket Policy and click Edit

Click Policy generator

Change Policy type to S3 Bucket Policy

Set Principle to *

Set Action to Get Object

Copy the S3 bucket ARN to add to the ARN field and add /* to the end.
eg: arn:aws:s3:::mfe-dashboard/*

Click Add Statement

Click Generate Policy

Copy paste the generated policy text to the Policy editor

Click Save changes


CloudFront setup
Go to AWS Management Console and use the search bar to find CloudFront

Click Create distribution

Set Origin domain to your S3 bucket

Find the Default cache behavior section and change Viewer protocol policy to Redirect HTTP to HTTPS

Scroll down and click Create Distribution

After Distribution creation has finalized click the Distribution from the list, find its Settings and click Edit

Scroll down to find the Default root object field and enter /container/latest/index.html

Click Save changes

Click Error pages

Click Create custom error response

Change HTTP error code to 403: Forbidden

Change Customize error response to Yes

Set Response page path to /container/latest/index.html

Set HTTP Response Code to 200: OK


Create IAM user
1. Search for "IAM"

2. Click "Create Individual IAM Users" and click "Manage Users"

3. Click "Add User"

4. Enter any name youâ€™d like in the "User Name" field.

5. Click "Next"

6. Click "Attach Policies Directly"

7. Use the search bar to find and tick AmazonS3FullAccess and CloudFrontFullAccess

8. Click "Next"

9. Click "Create user"

10. Select the IAM user that was just created from the list of users

11. Click "Security Credentials"

12. Scroll down to find "Access Keys"

13. Click "Create access key"

14. Select "Command Line Interface (CLI)"

15. Scroll down and tick the "I understand..." check box and click "Next"

16. Copy and/or download the Access Key ID and Secret Access Key to use for deployment.

Routing
-------

->  Navigation around subapps/microFEs is controlled from the container using its own routing logic
->  Navigation inside each microFE/subapp is built inside the subapp/microFE using its own routing logic
->  New routes added to the subApp/microFE shouldn't require a re-deploy of the container app
->  Navigation should work in both hosted and isolated mode
->  Route information shared between different microFEs should be done in a generic fashion like using callbacks not framework/library dependent.

So basically, the container route will simply decide which microFE to show, whereas each microFE should decide which page inside the micro FE app, it should show.

Routers maintain browserHistory, hashHistory and MemoryHistory.

Solution
--------

The Container/Host will use -> BrowserHistory. This is because we want only our container to have access to the browser address bar directly
since it has direct access to HTML5 History API when available, and falls back to full refreshes otherwise.
The BrowserRouter component from react-router-dom uses the browserHistory.
So if our container is using React, setting up React-router-dom in the container is enough for it to use the browerHistory.

The individual microFEs will use -> MemoryHistory. Basically the microFEs/subApps will have their own copy of what the current url is and thus
they can change that in any way they please without affecting what is there in the address bar. Basically they keep the URL changes in memory
and not in the user's browser.

Communication between MemoryRouter and BrowserRouter
----------------------------------------------------

To make things generic, all communications between different microFEs will happen via callbacks.
So communication between the BrowserRouter in container and MemoryRouter in the other microFEs will also happen via callbacks.

Container
    |
    |   pass a callback function say onNavigate
    |
   \ /
Marketing(MicroFE)

Now, whenever some clicks on a link say pricing in our marketing microFE, MemoryRouter will handle the routing in the marketing App.
So in memory of Marketing app, the path changes to /pricing. At the same time, we will call onNavigate to tell the container that the
current path has changed, so it needs to update the BrowserHistory (to show the changed url in the address bar)

** Important Point to consider for routing

If we don't set the publicPath property in our webpack.config file,
the scripts for our remote Apps will be loaded from the remoteEntry.js file relative to the URL that we loaded remoteEntry.js file from.

So, for example if we try to access marketing app from container w/o publicPath set, it will access the remoteEntry.js file
from the url: http://localhost:8081/remoteEntry.js to load the main.js file for marketingApp.

Same for our auth file which is hosted at localhost:8082, it tries to get the main.js file from http://localhost:8082/remoteEntry.js for the route /auth.
However, since we have provided the route as /auth/signin, it is now trying to look for the main.js file in http://localhost:8082/auth/remoteEntry.js and thus is not able to find the main.js file.
Now if we set up the publicPath for this (auth microFE), 

Authentication
--------------

Auth app will simply be used for signing in/signing out users.
We will not have the auth state managed inside it because if we do that, then it will need to loaded everytime, we need the auth state access.

To solve this we have 2 approaches:

-> Each subApp/microFE is aware of auth -> Results in Code Duplication
-> Centralize the auth state in the Container   -> Better solution